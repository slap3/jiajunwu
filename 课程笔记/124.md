
# 课程内容说明
#
### 特别说明：章节总结及代码标注一定程度上借助了chapgpt 4
###
##

1. **生成式人工智能的定义**：让机器产生复杂有结构的物件，如文章、语音、影像等，从近乎无穷的可能中找出适当组合，与分类（从有限选项中选择）不同。
2. **实现手段：**
   机器学习与深度学习。机器学习是机器自动从资料找函数，深度学习是更厉害的手段，今日的生成式人工智能多以深度学习达成，如ChatGPT、AI画图等模型都有上亿个参数。
3.  **生成策略:**
  语言模型：文章由文字构成，通过Autoregressive Generation依序生成文字。
图片生成：图片由像素构成，有类似语言模型的生成策略。
##
# prompt_part1
#
1. **生成式人工智能的能力**
    - **多领域应用**：如ChatGPT能进行文本生成、语言翻译等多种任务，功能广泛。
    - **工具进化**：可以处理各种不同类型的任务，且同一任务可能有多种解法。
2. **使用与应对策略**
    - **明确需求下达指令**：使用者应明确自己的需求，给人工智能下达清晰的指令，以获得更符合期望的结果，
    - **训练自己的模型**：训练自己的模型（如使用开源模型LLaMA），调整函数以满足特定需求。
3. **模型评估挑战**
    - **任务多样性与复杂性**：全面评估模型并不容易难度很大。
    - **内容管理**：模型需要防止说出有害内容，如脏话、抄袭、歧视等言论。
# prompt_part2
#
1. **强化语言模型的方法**
    - **拆解任务**
        - 可先拆解为多个步骤，模型还可检查自己的错误。
        - 同一问题模型每次答案可能不同，复杂任务可通过Self - Consistency等方法拆解成多步骤处理。
    - **使用工具**
        - **计算与信息获取**：语言模型有不擅长的事（如算数），但可使用工具。
        - **专业问题解答与创作辅助**：对于专业问题，模型可先搜索相关信息再回答，可利用工具辅助完成。
    - **模型合作**
        - **专业分工**：未来语言模型可专业分工，不同团队专注打造不同领域专业模型，让合适模型做合适任务。
        - **模型讨论与反思**：模型可彼此讨论、反省。
# prompt_part3
#
1. **模型合作方式**
    - **专业分工协作**：未来语言模型可进行专业分工，根据任务输入判断合适模型来完成任务模型合作能让合适模型做合适事情。
    - **模型相互讨论**
        - **讨论与反思**：模型间可相互讨论、反省。
        - **多模型讨论及规则**：增加讨论模型数量或轮次可能影响结果准确性。
2. **引入角色完成项目**
    - **团队角色设定**：引入不同角色来完成项目。
# LLMtraining_part1
#
1. **文字接龙学习基础**
    - **原理**：以未完成句子预测下一个token的方式进行文字接龙学习，模型（如Transformer）是有数十亿未知参数的函数，通过训练资料利用机器学习找出这些参数。
    - **阶段特点**
        - **自我学习，累积实力**：使用大量文字资料（如网络上各种文本）学习文字接龙，资料量可按需获取，人工介入较少。
        - **名帅指点，发挥潜力**：在之前的GPT系列中，模型大小和数据量不断发展。
        - **参与实战，打磨技巧**：语言模型虽从网络资料中学到很多，但还需要进一步训练来提升实际应用能力。
2. **训练面临的挑战与解决方法**
    - **挑战**
        - **找参数的挑战**：训练可能失败（找到的参数不符合训练资料），此时需更换超参数重新训练。
        - **模型合理性问题**：机器学习只管参数是否符合训练资料，不管是否合理）。
    - **解决方法**
        - **增加训练资料多样性**：可使找到的参数更合理。
        - **调整初始参数**：“train from scratch”通过随机设定初始参数，使最终参数比较接近初始化参数；“先验知识”则是寻找“好”的初始参数，使参数更可能合理，但“好”的初始参数获取存在困难。

# LLMtraining_part2
#
1. **人类老师教导与微调**
    - **督导式学习（Supervised Learning）**：通过人力进行资料标注，但人力成本高，无法收集太多资料。。
    - **微调路线**
        - **路线一：打造一堆专才**：针对不同任务（如翻译、编修等）分别训练专才模型。
        - **路线二：直接打造一个通才**：收集涵盖各种任务的大量标注资料训练通才模型。
2. **Instruction Fine - tuning的重要性及实践**
    - **重要性**：Instruction Fine - tuning被视为画龙点睛，高质量的Instruction Fine - tuning资料对提升模型效果至关重要，少量高质量标注数据即可达到较好效果。
    - **实践方法**：可以ChatGPT为参考进行逆向工程，先让ChatGPT想任务，再根据任务想可能输入，最后产生答案，如撰写邮件任务。
3. **模型发展与开源情况**
    - **模型开源**：Meta开源了LLaMA，在此基础上出现了Alpaca、Vicuna等模型，它们使用不同数据集、训练代码、评估指标，训练成本也有所不同。
    - **模型应用拓展**：进入人人可以fine - tune大型语言模型的时代，出现了众多基于LLaMA等的模型，有不同的数据使用和训练方式，部分模型还具有多模态能力。
# LLMtraining_part3
#
1. **强化学习（RLHF）概述**
    - **训练阶段**：大型语言模型训练分为三个阶段，第一阶段自我学习积累实力（如通过自督导式学习Self - supervised Learning），第二阶段名师指点发挥潜力（如督导式学习Supervised Learning中的Instruction Fine - tuning），第三阶段参与实战打磨技巧（如强化学习Reinforcement Learning中的RLHF）。
    - **RLHF原理**：语言模型在RLHF阶段进入新的“思考模式”，学习对生成结果做通盘考量。
2. **RLHF与Instruction Fine - tuning对比**
    - **Instruction Fine - tuning**：模型学习按照人类老师给定的正确答案进行接龙。
    - **RLHF优势**：在某些情况下，人类写出正确答案不易但容易判断好坏时，RLHF更适用。它使模型能从人类反馈中学习，生成更符合人类期望的结果。
3. **语言模型与AlphaGo对比**
    - **学习方式**
    - **语言模型**：第一阶段（Pre - train）和第二阶段（Instruction Fine - tuning）主要是人类老师说什么就跟着说什么，在RLHF阶段则需要人类反馈来优化。
    - **任务性质**：下围棋的每一步是分类问题，但整体来看也是生成式学习；语言模型则是通过生成文字来完成任务，如对话、回答问题等。
4. **RLHF的难题与待解议题**
    - **难题**：RLHF面临着如何平衡Helpfulness（有用性）和Safety（安全性）的难题。
    - **待解议题**：存在人类自己都无法正确判断好坏的状况，或者人的判断可能是错误的情况。
# agent
#
本文主要介绍了以大型语言模型打造的AI Agent，包括其现状、示例、与普通语言模型的区别、面临的挑战以及相关技术，具体内容如下：
1. **AI Agent概述**
    - **现状与期望**：目前多数人使用AI的方式较为单一，而未来人类期望AI Agent能更自主地完成任务。
2. **AI Agent与普通语言模型对比**
    - **普通语言模型的局限**：以ChatGPT为例，每次开始新对话时一切都重头来过，缺乏记忆功能。
    - **AI Agent的优势**：AI Agent具有记忆功能，能够在执行任务过程中积累经验，并根据经验调整行动。
3. **AI Agent面临的挑战与技术**
    - **挑战**：在执行任务时，AI Agent需要应对复杂多变的外界环境。
    - **技术**：涉及强化学习（Reinforcement Learning）相关技术。
# explain
#
本文主要探讨了大型语言模型的可解释性相关问题，包括其作为“黑盒子”的现状、如何使其可解释的方法、面临的问题及一些相关研究，具体内容如下：
1. **大型语言模型的“黑盒子”现状**
    - **透明度问题**：人们对其内部机制一无所知，尽管知道模型参数、训练资料与过程，但仍难以理解其决策过程。
    - **可解释性分类**：可解释性分为Interpretable（思维透明）和Explainable（可解释）。
2. **实现大型语言模型可解释性的方法**
    - **找出影响输出的关键输入**
        - **Gradient - based Approach**：通过观察每个输入的改变对输出的影响。
        - **分析Attention**：研究注意力机制，以确定输入中哪些部分对输出影响较大。
        - **In - context learning**：观察模型如何基于上下文学习和做出决策。
    - **找出影响输出的关键训练资料**：以对“shutdown”问题的回答为例，不同参数模型的回答受不同训练资料影响，较大模型有跨语言学习能力。
    - **分析Embedding中存储的信息**
        - **词性关系**：探究语言模型是否知道输入词汇的词性。
        - **可视化**：将信息投影到二维平面上以便于可视化分析。
3. **其他相关内容**
    - **语言模型的“测谎器”**：利用语言模型判断语句真假，通过概率分析得出结论。
    - **用AI解释AI**：可以用一个语言模型（如GPT - 4）来解释另一个语言模型（如GPT - 2）。
    - **直接询问语言模型**：语言模型会说话，可通过询问获取解释。
# transformer
#

1. **模型演进中的Transformer**
    - **模型发展历程**：语言模型经历了从N - gram、Feed - forward Network、Recurrent Neural Network（RNN）到Transformer的发展，Transformer是当前语言模型（如ChatGPT）的重要组成部分，本课程对Transformer的说明进行了简化，详细内容可参考过去课程。
2. **Transformer概述**
    - **工作流程**：Transformer主要包括Tokenization、Input Layer、Attention、Feed - Forward、Output Layer等部分，通过这些步骤实现文字接龙，即输入文字，将其转换为Token，理解Token（包括语义和位置信息），考虑上下文，进行整合、思考等操作后得到输出，可反复思考优化输出。
3. **Transformer各部分详解**
    - **Tokenization（把文字变成Token）**：语言模型以Token为单位处理文字。
    - **理解每个Token（语义和位置）**
        - **语义理解**：每个Token通过训练得到对应的向量（Embedding）。
        - **位置理解**：每个位置有独特的向量Positional Embedding，也是在训练时得到的参数，用于表示Token在文本中的位置信息。
    - **Attention（考虑上下文）**
        - **原理**：Attention机制主要贡献是发现不需要RNN，仅靠Attention即可。其工作过程包括找出相关Token并计算相关性，通过计算得到Attention Weight，集合相关信息。计算相关性时会得到Attention Matrix，实作时通常采用Causal Attention，只考虑左边的Token。此外，关联 性存在多种类型，如通过Multi - head Attention可从不同角度计算相关性并整合信息。
    - **Feed - Forward**：在Transformer Block中，经过Attention后会进行Feed - Forward操作，这部分内容在文档中未详细展开，但它是Transformer工作流程中的重要环节，与Attention等部分共同协作实现模型功能，多个Transformer Block（如Layer 1、Layer 2等）依次处理信息，最后通过Output Layer（包含Linear Transformer和Softmax）得到最终输出。
# evaluation
#
本文主要讲述了语言模型能力的评估相关内容，包括评估方式、面临的问题、不同的评估基准以及其他相关考虑因素，具体如下：
1. **语言模型能力评估方式**
    - **与标准答案对比**：通过将模型的输出与标准答案进行对比来评估模型能力，与标准答案不同并不一定代表错误，评估指标如BLEU（用于翻译）、ROUGE（用于摘要）主要做字面比对。
    - **人工评估与模型评估**：人工评估可能相对准确，但耗时费力；也可以用强大的语言模型来评估其他模型（如MT - Bench）。
2. **评估面临的问题**
    - **答案的多样性与模糊性**：如选择题中模型可能输出不规范，对于一些问题可能存在多种合理答案或难以界定绝对正确的答案，导致评估困难。
    - **模型的不确定性**：语言模型的输出具有一定随机性，且可能受到训练数据、模型本身特点等因素影响，使得评估结果不稳定。
    - **其他方面**：除了评估模型效能，还需考虑人工智慧的安全性（如防止模型唬烂、被骗、产生偏见、抄袭等），同时模型的价格、速度等也是实际应用中需要考虑的因素。
# ethical
#
本文主要讲述了大型语言模型的安全性议题，包括语言模型的错误类型、评估偏倚的方法、政治倾向、减轻偏倚的措施、检测AI生成内容的方法以及语言模型在学术领域应用中的问题，具体内容如下：
1. **语言模型的错误与应对措施**
    - **错误类型 - Hallucination**：语言模型会犯错，如产生幻觉（输出与事实不符的内容）。
    - **应对错误 - 事实核查示例**：尽管有些模型会进行事实核查，但仍可能提供与事实不完全相符或有遗漏的信息。
2. **语言模型的偏倚评估**
    - **评估方法**
        - **情感分析差异**：通过对比不同文本在语言模型中的情感分析得分差距，来初步判断模型是否存在偏倚。
        - **红队测试**：由专门负责想产生偏见输入的团队（红队），通过修改性别相关词汇等方式，利用强化学习最大化差距，评估模型在性别等方面的偏倚情况。
    - **偏倚表现**
        - **职业性别刻板印象**：在生成职业表现反馈时，语言模型（如ChatGPT）对不同职业（如幼儿园老师和建筑工人）使用的代词存在差异，体现出对职业性别的刻板印象。
        - **种族歧视争议**：在审查履历时，语言模型（如GPT）对不同种族和性别的名字存在偏好差异，引发种族歧视争议。
3. **语言模型的政治倾向**
    - **倾向测试**：通过让语言模型选择对政治观点的态度选项，以及利用相关工具进行政治倾向测试，发现不同模型倾向不一。
4. **减轻偏倚的方法**
    - **方法分类**：主要包括预处理（Pre - Processing，改变模型输入，如训练数据或提示）、训练中处理、处理中和后处理，详细内容可参考概述论文。
5**语言模型输出浮水印**
    - **浮水印概念**：为解决语言模型输出内容难以辨别来源的问题，可在模型输出中加上人类难以辨识的暗号。
# injection
#
1. **Jailbreaking**
    - **概念与攻击对象**：针对语言模型本身，试图让语言模型说出不该讲的话，类比于人类社会中的杀人放火等违反规则的行为。
    - **攻击方式**
        - **使用不熟悉语言**：如用特定语言（非通用语言）询问如何砍倒停车标志，模型可能会给出工具和步骤，而正常情况下应拒绝回复。
        - **给予冲突指令**：以特定开头要求回答砍倒停车标志所需工具，模型可能会提供相关工具信息，违背正常的安全和法律原则。
        - **试图说服模型**：通过编写停车标志作恶的故事等方式试图说服模型，使其在面对非法问题（如砍倒停车标志）时提供帮助。
    - **目的**：除了获取非法或不适当信息外，还可用于训练数据重建，如获取个人隐私信息或让模型执行异常操作。
2. **Prompt Injection（提示注入）**
    - **概念与攻击对象**：针对以语言模型打造的应用（如AI助教），让语言模型在不恰当时机做不恰当事情。
    - **攻击示例**：在AI助教评估文章作业的场景中，通过特定指令，使模型偏离正常评估任务，给出不符合实际的分数。
    - **相关比赛**：有Prompt Injection比赛，涉及多种攻击技术（如Context Overflow、Compound Instruction、Cognitive Hacking等，具体如递归、特殊情况处理、风格注入、上下文忽略、虚拟化、代码注入、语法转换等多种方式。
# strategy
#
1. **生成式人工智能基础**
    - **构成要素**：生成式人工智能旨在让机器产生复杂有结构的物件，如文字、影像、声音等。
    - **本质**：将基本单位按正确排序组合，在不同应用场景（如绘图AI、对话AI、声音生成AI等）下，根据给定条件生成相应内容。
2. **Autoregressive Generation（AR）策略**
    - **原理与应用**：按顺序逐个生成基本单位，在文字生成上非常成功，如语言模型根据前文依次生成后续内容。也可应用于影像（如Image GPT）和声音（如WavNet）生成，但本质上需要按部就班。
3. **Non - autoregressive Generation（NAR）策略**
    - **原理与特点**：可平行运算，一次生成所有基本单位，速度上有优势，但生成质量存在问题。。
4. **Autoregressive与Non - autoregressive结合策略**
    - **结合方式与优势**：先用Autoregressive生成精简版本，再用Non - autoregressive生成精细版本。
5. **Speculative Decoding优化策略**
    - **原理与速度提升**：通过引入预言家（可由快速但可能犯错的模型如Non - autoregressive Model或Compressive Model担任）来预判语言模型接下来会说的内容，从而提高生成速度。也可采用多个预言家来提高预测准确性。
# vision
#
1. **影像处理基础**
    - **影像构成**：图片由像素构成，影片由一帧一帧图片构成。
    - **人工智能对影像的处理**：通过编码器（Encoder）和解码器（Decoder）处理。
2. **生成策略**
- **以文字为条件**：如Sora模型，可根据文字描述生成动画场景、影片等。
     - **生成方式**：以文字生图为例，模型（如Transformer）会对文字进行处理，生成图片的各个部分（如将“一只在奔跑的狗”拆分为多个部分平行生成，最终组合）等。
    - **影像生影像**：包括影片完成（如生成连续的帧）、风格转换、画质提升等，可通过ControlNet等技术实现。
    - **其他输入生影像**：如Talking Head，可根据上传的参考图像生成相关影像。
3. **评估方法**：使用CLIP等工具评估影像生成的好坏，根据生成影像与文字描述的匹配程度等给予评分。
4. **面临挑战**
    - **文字生影片挑战**：计算量极其巨大。
    - **脑补问题**：生成影像时模型可能脑补过多导致结果不确定。
5. **互动性探索**：如Genie项目尝试实现与生成影像的更强互动，可收集大量游戏影片作为训练资料，从之前游戏画面抽取信息预测下一个画面及动作。

